{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first suspect to plead guilty in Singapore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta has been fined a record-breaking €1.2 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SINGAPORE: A 45-year-old man linked to Singapo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Department of Education imposed a record $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>METHODOLOGY\\n\\nThis investigation was conducte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>In February, 2002, DPKO Communications and Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>\"Another proposed topic was efforts by the Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>\"INTRODUCTION\\n\\nThis case arises out of an au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>\"Description\\n\\nInterception of French Ambassa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1524 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text\n",
       "0     Starbucks violated federal labor law when it i...\n",
       "1     The first suspect to plead guilty in Singapore...\n",
       "2     Meta has been fined a record-breaking €1.2 bil...\n",
       "3     SINGAPORE: A 45-year-old man linked to Singapo...\n",
       "4     The Department of Education imposed a record $...\n",
       "...                                                 ...\n",
       "1519  METHODOLOGY\\n\\nThis investigation was conducte...\n",
       "1520  In February, 2002, DPKO Communications and Tec...\n",
       "1521  \"Another proposed topic was efforts by the Eur...\n",
       "1522  \"INTRODUCTION\\n\\nThis case arises out of an au...\n",
       "1523  \"Description\\n\\nInterception of French Ambassa...\n",
       "\n",
       "[1524 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/combined_news_excerpts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Path</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.pdf</td>\n",
       "      <td>Pristina Airport – Possible administrative irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.pdf</td>\n",
       "      <td>Investigative details\\n\\nIn his/her interviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"An interoffice memorandum providing an “outst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"Allegation 2 &amp; 3:\\n\\n(Specifically, three of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"When asked about this in interview, the Divis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>89.pdf</td>\n",
       "      <td>\"Description\\n\\nTop Secret US National Securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>9.pdf</td>\n",
       "      <td>\"INTRODUCTION\\n\\nThis case arises out of an au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>9.pdf</td>\n",
       "      <td>\"BACKGROUND INFORMATION\\n\\nPristina Internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>9.pdf</td>\n",
       "      <td>\"BACKGROUND INFORMATION\\n\\nPristina Internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>91.pdf</td>\n",
       "      <td>\"Description\\n\\nInterception of French Ambassa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PDF Path                                               Text\n",
       "0      1.pdf  Pristina Airport – Possible administrative irr...\n",
       "1      1.pdf  Investigative details\\n\\nIn his/her interviews...\n",
       "2     10.pdf  \"An interoffice memorandum providing an “outst...\n",
       "3     10.pdf  \"Allegation 2 & 3:\\n\\n(Specifically, three of ...\n",
       "4     10.pdf  \"When asked about this in interview, the Divis...\n",
       "..       ...                                                ...\n",
       "138   89.pdf  \"Description\\n\\nTop Secret US National Securit...\n",
       "139    9.pdf  \"INTRODUCTION\\n\\nThis case arises out of an au...\n",
       "140    9.pdf  \"BACKGROUND INFORMATION\\n\\nPristina Internatio...\n",
       "141    9.pdf  \"BACKGROUND INFORMATION\\n\\nPristina Internatio...\n",
       "142   91.pdf  \"Description\\n\\nInterception of French Ambassa...\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_excel('./dataset/wikileaks_parsed.xlsx')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_keywords_spacy(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Tokenize using spaCy, remove stopwords/punctuation,\n",
    "    lemmatize tokens, and then pick top_n frequent terms.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower().strip())\n",
    "    \n",
    "    # Filter out stopwords, punctuation, small tokens, etc.\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if (\n",
    "            not token.is_stop             # skip common stopwords\n",
    "            and not token.is_punct        # skip punctuation\n",
    "            and not token.like_num        # skip pure numbers\n",
    "            and len(token) > 2            # skip tiny tokens like 'of', 'is'\n",
    "        ):\n",
    "            # Use the lemma form (e.g., \"attacks\" -> \"attack\")\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    # Count frequencies\n",
    "    freq = {}\n",
    "    for w in filtered_tokens:\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "    \n",
    "    # Sort by highest frequency\n",
    "    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top_n words as a set (for intersection usage)\n",
    "    top_words = [word for (word, _) in sorted_words[:top_n]]\n",
    "    return set(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc_ID_News Doc_ID_Wiki  Similarity_Score Common_Entities Shared_Keywords\n",
      "0        N000        W000              0.01               —               —\n",
      "1        N000        W001              0.02               —               —\n",
      "2        N000        W002              0.00               —               —\n",
      "3        N000        W003              0.00               —               —\n",
      "4        N000        W004              0.00               —               —\n",
      "5        N000        W005              0.00               —               —\n",
      "6        N000        W006              0.01               —               —\n",
      "7        N000        W007              0.00               —               —\n",
      "8        N000        W008              0.01               —               —\n",
      "9        N000        W009              0.00               —               —\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#########################\n",
    "# 1) Load Data\n",
    "#########################\n",
    "news_file = './dataset/combined_news_excerpts.csv'\n",
    "wikileaks_file = './dataset/wikileaks_parsed.xlsx'\n",
    "\n",
    "news_df = pd.read_csv(news_file)         # Should have columns: [Doc_ID, Text]\n",
    "wiki_df = pd.read_excel(wikileaks_file)  # Should have columns: [Doc_ID, Text]\n",
    "\n",
    "# If you don't have an existing Doc_ID, generate them:\n",
    "# Example: N000, N001... and W000, W001...\n",
    "if 'Doc_ID' not in news_df.columns:\n",
    "    news_df['Doc_ID'] = [\"N\" + str(i).zfill(3) for i in news_df.index]\n",
    "if 'Doc_ID' not in wiki_df.columns:\n",
    "    wiki_df['Doc_ID'] = [\"W\" + str(i).zfill(3) for i in wiki_df.index]\n",
    "\n",
    "#########################\n",
    "# 2) Basic Preprocessing\n",
    "#########################\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)      # remove excess whitespace\n",
    "    return text\n",
    "\n",
    "news_df['clean_text'] = news_df['Text'].apply(preprocess_text)\n",
    "wiki_df['clean_text'] = wiki_df['Text'].apply(preprocess_text)\n",
    "\n",
    "#########################\n",
    "# 3) Load spaCy Model\n",
    "#########################\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#########################\n",
    "# 4) Improved Keyword Extraction\n",
    "#    - Remove stopwords, punctuation, short tokens\n",
    "#    - Lemmatize tokens\n",
    "#########################\n",
    "def get_keywords_spacy(text, top_n=5):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Filter out stopwords, punctuation, numeric strings, etc.\n",
    "        if (\n",
    "            not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.like_num\n",
    "            and len(token) > 2\n",
    "        ):\n",
    "            tokens.append(token.lemma_.lower())  # lemmatize\n",
    "    # Frequency count\n",
    "    freq = {}\n",
    "    for w in tokens:\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "    # sort by frequency desc\n",
    "    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word for word, _ in sorted_words[:top_n]]\n",
    "    return set(top_words)\n",
    "\n",
    "news_df['keywords'] = news_df['clean_text'].apply(get_keywords_spacy)\n",
    "wiki_df['keywords'] = wiki_df['clean_text'].apply(get_keywords_spacy)\n",
    "\n",
    "#########################\n",
    "# 5) Named Entity Extraction\n",
    "#    Filter to relevant entity labels\n",
    "#########################\n",
    "allowed_labels = {\"ORG\", \"PERSON\", \"GPE\", \"LOC\", \"NORP\"}\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in allowed_labels:\n",
    "            entities.add(ent.text.strip())\n",
    "    return entities\n",
    "\n",
    "news_df['entities'] = news_df['clean_text'].apply(extract_entities)\n",
    "wiki_df['entities'] = wiki_df['clean_text'].apply(extract_entities)\n",
    "\n",
    "#########################\n",
    "# 6) TF-IDF Vector Calculation\n",
    "#    Use ngram_range to capture some phrases\n",
    "#########################\n",
    "all_texts = news_df['clean_text'].tolist() + wiki_df['clean_text'].tolist()\n",
    "\n",
    "# Example: use n-grams up to 2 to catch short phrases\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "num_news = len(news_df)\n",
    "num_wiki = len(wiki_df)\n",
    "\n",
    "# Split into two sub-matrices\n",
    "news_vectors = tfidf_matrix[:num_news]\n",
    "wiki_vectors = tfidf_matrix[num_news:]\n",
    "\n",
    "#########################\n",
    "# 7) Cosine Similarities\n",
    "#########################\n",
    "similarity_scores = cosine_similarity(news_vectors, wiki_vectors)\n",
    "\n",
    "#########################\n",
    "# 8) Build Final Similarity Table\n",
    "#########################\n",
    "rows = []\n",
    "\n",
    "for i, news_row in news_df.iterrows():\n",
    "    for j, wiki_row in wiki_df.iterrows():\n",
    "        sim_score = similarity_scores[i, j]\n",
    "        \n",
    "        # Common Entities\n",
    "        common_ents = news_row['entities'].intersection(wiki_row['entities'])\n",
    "        common_ents_str = \"; \".join(common_ents) if common_ents else \"—\"\n",
    "        \n",
    "        # Shared Keywords\n",
    "        shared_kws = news_row['keywords'].intersection(wiki_row['keywords'])\n",
    "        shared_kws_str = \"; \".join(shared_kws) if shared_kws else \"—\"\n",
    "        \n",
    "        row_data = {\n",
    "            \"Doc_ID_News\": news_row['Doc_ID'],\n",
    "            \"Doc_ID_Wiki\": wiki_row['Doc_ID'],\n",
    "            \"Similarity_Score\": round(sim_score, 2),\n",
    "            \"Common_Entities\": common_ents_str,\n",
    "            \"Shared_Keywords\": shared_kws_str\n",
    "        }\n",
    "        rows.append(row_data)\n",
    "\n",
    "similarity_df = pd.DataFrame(rows)\n",
    "\n",
    "#########################\n",
    "# 9) Preview & Save\n",
    "#########################\n",
    "print(similarity_df.head(10))\n",
    "\n",
    "# Save to CSV if needed\n",
    "# similarity_df.to_csv(\"similarity_table.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID_News</th>\n",
       "      <th>Doc_ID_Wiki</th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>Common_Entities</th>\n",
       "      <th>Shared_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217931</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the un oil­for­food (off); french; nzl; iraq; ...</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212949</th>\n",
       "      <td>N1489</td>\n",
       "      <td>W022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>italian; europe; washington; east jerusalem; n...</td>\n",
       "      <td>italian; italy; help; israel; relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215588</th>\n",
       "      <td>N1507</td>\n",
       "      <td>W087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>unmik; kosovo; the kosovo force; united nation...</td>\n",
       "      <td>airport; include; investigation; own; pristina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216323</th>\n",
       "      <td>N1512</td>\n",
       "      <td>W107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kosovo; unmik; doti; united nations security c...</td>\n",
       "      <td>official; airport; doti; pristina; pillar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215002</th>\n",
       "      <td>N1503</td>\n",
       "      <td>W073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the kosovo trust agency; kosovo; london</td>\n",
       "      <td>fund; airport; insurance; coverage; manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103646</th>\n",
       "      <td>N724</td>\n",
       "      <td>W114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103647</th>\n",
       "      <td>N724</td>\n",
       "      <td>W115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103648</th>\n",
       "      <td>N724</td>\n",
       "      <td>W116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103649</th>\n",
       "      <td>N724</td>\n",
       "      <td>W117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108966</th>\n",
       "      <td>N762</td>\n",
       "      <td>W000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217932 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc_ID_News Doc_ID_Wiki  Similarity_Score  \\\n",
       "217931       N1523        W142               1.0   \n",
       "212949       N1489        W022               1.0   \n",
       "215588       N1507        W087               1.0   \n",
       "216323       N1512        W107               1.0   \n",
       "215002       N1503        W073               1.0   \n",
       "...            ...         ...               ...   \n",
       "103646        N724        W114               0.0   \n",
       "103647        N724        W115               0.0   \n",
       "103648        N724        W116               0.0   \n",
       "103649        N724        W117               0.0   \n",
       "108966        N762        W000               0.0   \n",
       "\n",
       "                                          Common_Entities  \\\n",
       "217931  the un oil­for­food (off); french; nzl; iraq; ...   \n",
       "212949  italian; europe; washington; east jerusalem; n...   \n",
       "215588  unmik; kosovo; the kosovo force; united nation...   \n",
       "216323  kosovo; unmik; doti; united nations security c...   \n",
       "215002            the kosovo trust agency; kosovo; london   \n",
       "...                                                   ...   \n",
       "103646                                                  —   \n",
       "103647                                                  —   \n",
       "103648                                                  —   \n",
       "103649                                                  —   \n",
       "108966                                                  —   \n",
       "\n",
       "                                       Shared_Keywords  \n",
       "217931              report; french; company; u.s; plan  \n",
       "212949          italian; italy; help; israel; relation  \n",
       "215588  airport; include; investigation; own; pristina  \n",
       "216323       official; airport; doti; pristina; pillar  \n",
       "215002     fund; airport; insurance; coverage; manager  \n",
       "...                                                ...  \n",
       "103646                                               —  \n",
       "103647                                               —  \n",
       "103648                                               —  \n",
       "103649                                               —  \n",
       "108966                                               —  \n",
       "\n",
       "[217932 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df.sort_values('Similarity_Score', ascending=False, inplace=True)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>National Labor Relations Board</td>\n",
       "      <td>ORG</td>\n",
       "      <td>127</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NLRB</td>\n",
       "      <td>ORG</td>\n",
       "      <td>223</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>249</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>422</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14596</th>\n",
       "      <td>1523</td>\n",
       "      <td>FTC</td>\n",
       "      <td>ORG</td>\n",
       "      <td>212</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14597</th>\n",
       "      <td>1523</td>\n",
       "      <td>Avast</td>\n",
       "      <td>ORG</td>\n",
       "      <td>221</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14598</th>\n",
       "      <td>1523</td>\n",
       "      <td>Avast</td>\n",
       "      <td>ORG</td>\n",
       "      <td>430</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14599</th>\n",
       "      <td>1523</td>\n",
       "      <td>Czechoslovakia</td>\n",
       "      <td>GPE</td>\n",
       "      <td>513</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14600</th>\n",
       "      <td>1523</td>\n",
       "      <td>the Soviet Bloc</td>\n",
       "      <td>GPE</td>\n",
       "      <td>548</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14601 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dataset                          Entity Type  Start  End\n",
       "0            0                       Starbucks  ORG      0    9\n",
       "1            0  National Labor Relations Board  ORG    127  157\n",
       "2            0                            NLRB  ORG    223  227\n",
       "3            0                       Starbucks  ORG    249  258\n",
       "4            0                       Starbucks  ORG    422  431\n",
       "...        ...                             ...  ...    ...  ...\n",
       "14596     1523                             FTC  ORG    212  215\n",
       "14597     1523                           Avast  ORG    221  226\n",
       "14598     1523                           Avast  ORG    430  435\n",
       "14599     1523                  Czechoslovakia  GPE    513  527\n",
       "14600     1523                 the Soviet Bloc  GPE    548  563\n",
       "\n",
       "[14601 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df = pd.read_excel('./entities_dashboard_filtered.xlsx')\n",
    "entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc_ID_News Doc_ID_Wiki  Similarity_Score            Common_Entities  \\\n",
      "0       N1523        W142               1.0  the un oil­for­food (off)   \n",
      "1       N1523        W142               1.0                     french   \n",
      "2       N1523        W142               1.0                        nzl   \n",
      "3       N1523        W142               1.0                       iraq   \n",
      "4       N1523        W142               1.0                 washington   \n",
      "5       N1523        W142               1.0                     ts//si   \n",
      "6       N1523        W142               1.0                       u.s.   \n",
      "7       N1523        W142               1.0              david levitte   \n",
      "8       N1523        W142               1.0                         un   \n",
      "9       N1523        W142               1.0                     france   \n",
      "\n",
      "                      Shared_Keywords  \n",
      "0  report; french; company; u.s; plan  \n",
      "1  report; french; company; u.s; plan  \n",
      "2  report; french; company; u.s; plan  \n",
      "3  report; french; company; u.s; plan  \n",
      "4  report; french; company; u.s; plan  \n",
      "5  report; french; company; u.s; plan  \n",
      "6  report; french; company; u.s; plan  \n",
      "7  report; french; company; u.s; plan  \n",
      "8  report; french; company; u.s; plan  \n",
      "9  report; french; company; u.s; plan  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suppose you have a DataFrame called similarity_df, with a column \"Common_Entities\"\n",
    "# that contains values like: \"the united states; kosovo; UNMIK\"\n",
    "\n",
    "# 1) Convert semicolon‐separated strings to lists\n",
    "similarity_df[\"Common_Entities\"] = (\n",
    "    similarity_df[\"Common_Entities\"]\n",
    "    .fillna(\"\")  # handle NaNs if any\n",
    "    .apply(lambda x: [ent.strip() for ent in str(x).split(';') if ent.strip()])\n",
    ")\n",
    "\n",
    "# 2) Explode\n",
    "# This creates multiple rows for each item in the list\n",
    "similarity_exploded = similarity_df.explode(\"Common_Entities\").reset_index(drop=True)\n",
    "\n",
    "# Now each row has exactly ONE entity in \"Common_Entities\"\n",
    "print(similarity_exploded.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID_News</th>\n",
       "      <th>Doc_ID_Wiki</th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>Common_Entities</th>\n",
       "      <th>Shared_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the un oil­for­food (off)</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>french</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nzl</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>iraq</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1523</td>\n",
       "      <td>W142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>washington</td>\n",
       "      <td>report; french; company; u.s; plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219163</th>\n",
       "      <td>N724</td>\n",
       "      <td>W114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219164</th>\n",
       "      <td>N724</td>\n",
       "      <td>W115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219165</th>\n",
       "      <td>N724</td>\n",
       "      <td>W116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219166</th>\n",
       "      <td>N724</td>\n",
       "      <td>W117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219167</th>\n",
       "      <td>N762</td>\n",
       "      <td>W000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219168 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc_ID_News Doc_ID_Wiki  Similarity_Score            Common_Entities  \\\n",
       "0            N1523        W142               1.0  the un oil­for­food (off)   \n",
       "1            N1523        W142               1.0                     french   \n",
       "2            N1523        W142               1.0                        nzl   \n",
       "3            N1523        W142               1.0                       iraq   \n",
       "4            N1523        W142               1.0                 washington   \n",
       "...            ...         ...               ...                        ...   \n",
       "219163        N724        W114               0.0                          —   \n",
       "219164        N724        W115               0.0                          —   \n",
       "219165        N724        W116               0.0                          —   \n",
       "219166        N724        W117               0.0                          —   \n",
       "219167        N762        W000               0.0                          —   \n",
       "\n",
       "                           Shared_Keywords  \n",
       "0       report; french; company; u.s; plan  \n",
       "1       report; french; company; u.s; plan  \n",
       "2       report; french; company; u.s; plan  \n",
       "3       report; french; company; u.s; plan  \n",
       "4       report; french; company; u.s; plan  \n",
       "...                                    ...  \n",
       "219163                                   —  \n",
       "219164                                   —  \n",
       "219165                                   —  \n",
       "219166                                   —  \n",
       "219167                                   —  \n",
       "\n",
       "[219168 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Entity Labels DF (Standardise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels_df = pd.read_excel('./entities_dashboard_filtered.xlsx')\n",
    "entity_labels_df['Entity']= entity_labels_df['Entity'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Merges to get Entity_Type for each Entity_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose entity_labels_df has columns: [Entity_Name, Entity_Type]\n",
    "# Example: \"the united states\" -> GPE, \"UNMIK\" -> ORG\n",
    "\n",
    "similarity_exploded = similarity_exploded.merge(\n",
    "    entity_labels_df,\n",
    "    left_on=\"Common_Entities\",\n",
    "    right_on=\"Entity\",\n",
    "    how=\"left\")\n",
    "\n",
    "# Drop the repeated \"Entity_Name\" column after merge\n",
    "# similarity_exploded.drop(columns=[\"Entity\"], inplace=True)\n",
    "\n",
    "# Now you have columns like:\n",
    "# [Doc_ID_News, Doc_ID_Wiki, Similarity_Score, Common_Entities, Entity_Type, Shared_Keywords, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_exploded.drop(columns=['Dataset', 'Start', 'End'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_exploded.rename(columns={\n",
    "    'Type': 'Common_Entities_Type'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_exploded.to_csv('./dataset/tableau_chart1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "\n",
    "# Exploding the 'Shared_Keywords' into separate rows\n",
    "similarity_exploded['Shared_Keywords'] = similarity_exploded['Shared_Keywords'].str.split('; ')\n",
    "exploded_df = similarity_exploded.explode('Shared_Keywords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.drop(columns=['category_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.to_csv('transformed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
